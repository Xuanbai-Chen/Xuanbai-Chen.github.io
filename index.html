<!DOCTYPE HTML>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xuanbai Chen</title>
  <meta name="author" content="Xuanbai Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">


        <!--bio paragraph-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xuanbai Chen</name>
              </p>
                  <p>Xuanbai Chen is currently an Applied Scientist in <a href="https://aws.amazon.com/bedrock/bda/">AWS AI Computer Vision</a> team joining in July 2023.
<!--              <p>Xuanbai Chen is currently an Applied Scientist in <a href="https://aws.amazon.com/bedrock/bda/">AWS AI Computer Vision</a> team joining in July 2023, and is working on face recognition and trustworthy AI.-->
              </p>
              <p>
                Prior to AWS, he obtained his <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">Master of Science in Computer Vision (MSCV)</a> Degree
                in the <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cs.cmu.edu/">School of Computer Science</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a> in May 2023.
<!--                He worked with <a href="https://www.cs.cmu.edu/~ftorre/">Prof. Fernando De la Torre</a> on Fairness for Generation Model and 3D Face modeling.-->
                He obtained my Bachelor of Engineering in Computer Science (B.Eng in CS) Degree in <a href="https://cc.nankai.edu.cn/">College of Computer Science</a>, <a href="https://en.nankai.edu.cn/">Nankai University</a> in June 2021.
<!--                , working on Video Summarization.-->
              </p>
              <p style="text-align:center">
                <a href="mailto:xuanbaic98@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xuanbaic/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=UZ8IbowAAAAJ">Google Scholar </a>
<!--                &nbsp/&nbsp-->
<!--                <a href="https://www.overleaf.com/read/swvmmvvqfhqy">CV</a>-->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/xuanbaic.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/xuanbaic.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>




<!--        &lt;!&ndash;News&ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--          <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--            <heading>News</heading>-->
<!--            <p>-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2024-07:</b> Our <a href="https://ieeexplore.ieee.org/abstract/document/10687758"> paper</a> for video summarization has been accepted by <a href="https://2024.ieeeicme.org/">ICME 2024</a>.-->
<!--              </li>-->
<!--              <li style="margin: 5px;" >-->
<!--                  <b>2024-03:</b> Our <a href="https://xsciencedirect.com/science/article/pii/S0950705124003058"> paper</a> for video summarization has been accepted by <a href="https://www.sciencedirect.com/journal/knowledge-based-systems">KBS</a>.-->
<!--              </li>-->
<!--              <li style="margin: 5px;" >-->
<!--                  <b>2024-02:</b> Our <a href="https://hcplayercvpr2024.github.io/"> paper</a> in better generating human images has been accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.-->
<!--              </li>-->
<!--              <li style="margin: 5px;" >-->
<!--                  <b>2023-10:</b> ITI-GEN has entered the <b>Best Paper Finalist</b> (17 out of 2161 accepted paper <b>Top 0.7%</b>, 8260 submissions <b>Top 0.2%</b>) for ICCV 2023.-->
<!--              </li>-->
<!--              <li style="margin: 5px;" >-->
<!--                  <b>2023-08:</b> Our <a href="https://czhang0528.github.io/iti-gen"> ITI-GEN</a> has been accepted by <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> as an <b>Oral</b> paper.-->
<!--              </li>-->
<!--              <li style="margin: 5px;" >-->
<!--                <b>2023-07:</b> I joined <a href="https://aws.amazon.com/rekognition/">AWS AI Rekognition</a> team as an Applied Scientist!-->
<!--              </li>-->
<!--            </p>-->
<!--          </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



<!--        &lt;!&ndash;publications&ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Publications</heading> (* indicates equal contribution)-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



<!--        &lt;!&ndash; AMF  &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/amf/figure.png", width="170">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/10687758">-->
<!--                <span class="papertitle">An Aesthetic-Guided Multimodal Framework for Video Summarization</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.semanticscholar.org/author/Jiehang-Xie/147178951">Jiehang Xie</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=ObO3hW8AAAAJ">Shao-Ping Lu</a>-->
<!--              <br>-->
<!--              <em>ICME</em>, 2024-->
<!--              <br>-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/10687758">project page</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Propose a novel video summarization framework based on aesthetic guidance and multimodal information.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



<!--        &lt;!&ndash; MANVS  &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/kamn/fig.png", width="170">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://xsciencedirect.com/science/article/pii/S0950705124003058">-->
<!--                <span class="papertitle">Video Summarization via Knowledge-Aware Multimodal Deep Networks</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.semanticscholar.org/author/Jiehang-Xie/147178951">Jiehang Xie</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=LJiQRJIAAAAJ">Sicheng Zhao</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=ObO3hW8AAAAJ">Shao-Ping Lu</a>-->
<!--              <br>-->
<!--              <em>KBS</em>, 2024-->
<!--              <br>-->
<!--              <a href="https://xsciencedirect.com/science/article/pii/S0950705124003058">project page</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Propose a novel video summarization approach based on a knowledge-aware multimodal network.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->


<!--        &lt;!&ndash;   human centric prior     &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/human_prior/1.png", width="200">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://hcplayercvpr2024.github.io/">-->
<!--                <span class="papertitle">Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://scholar.google.com/citations?user=5yS_tTUAAAAJ">Junyan Wang</a>,-->
<!--              <a href="https://scholar.google.com.hk/citations?user=eDiXHP8AAAAJ">Zhenhong Sun</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=XprTQQ8AAAAJ">Zhiyu Tan</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=KWVlYaMAAAAJ">Weihua Chen</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=pHN-QIwAAAAJ">Hao Li</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=vb3l1ZMAAAAJ">Cheng Zhang</a>,-->
<!--              <a href="https://scholar.google.com.au/citations?user=7u3M9hMAAAAJ">Yang Song</a>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2024-->
<!--              <br>-->
<!--              <a href="https://hcplayercvpr2024.github.io/">project page</a>-->
<!--              /-->
<!--              <a href="https://arxiv.org/abs/2403.05239">arXiv</a>-->
<!--              /-->
<!--              <a href="https://github.com/hcplayercvpr2024/hcplayer">code</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Integrating human-centric priors to help t2i model better generate human images.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->





<!--        &lt;!&ndash;   iti-gen     &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/iti_gen/teaser.png", width="200">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.html">-->
<!--                <span class="papertitle">ITI-GEN: Inclusive Text-to-Image Generation</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://scholar.google.com/citations?user=vb3l1ZMAAAAJ">Cheng Zhang</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.at/citations?user=vmarc-AAAAAJ">Siqi Chai</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=WFKit_4AAAAJ">Chen Henry Wu</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=sY8lt7AAAAAJ">Dmitry Lagun</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=q5y44h8AAAAJ">Thabo Beeler</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=YB8_6gkAAAAJ">Fernando De la Torre</a>-->
<!--              <br>-->
<!--              <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist, 17 out of 8260 submissions)</strong></font>-->
<!--              <br>-->
<!--              <a href="https://czhang0528.github.io/iti-gen">project page</a>-->
<!--              /-->
<!--              <a href="https://arxiv.org/abs/2309.05569">arXiv</a>-->
<!--              /-->
<!--              <a href="https://github.com/humansensinglab/ITI-GEN">code</a>-->
<!--              /-->
<!--              <a href="https://drive.google.com/drive/folders/1_vwgrcSq6DKm5FegICwQ9MwCA63SkRcr?usp=sharing">data</a>-->
<!--              /-->
<!--              <a href="images/iti_gen/iti-gen_poster.pdf">poster</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Debias the text-to-image generation model and implement the inclusiveness.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



<!--        &lt;!&ndash;   3d face cam     &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/3dfacecam/3dfacecam.gif", width="200">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://openaccess.thecvf.com/content/WACV2023/html/Taherkhani_Controllable_3D_Generative_Adversarial_Face_Model_via_Disentangling_Shape_and_WACV_2023_paper.html">-->
<!--                <span class="papertitle">Controllable 3D Generative Adversarial Face Model via Disentangling Shape and Appearance</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://scholar.google.com/citations?user=Lk8MgHIAAAAJ">Fariborz Taherkhani</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=RxaDP08AAAAJ">Aashish Rai</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=tIdThSIAAAAJ">Quankai Gao</a>,-->
<!--              <a href="https://shaunak99.github.io/profilev2/">Shaunak Srivastava</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=YB8_6gkAAAAJ">Fernando De la Torre</a>,-->
<!--              Steven Song,-->
<!--              <a href="https://scholar.google.com/citations?user=hUXGzpgAAAAJ">Aayush Prakash</a>,-->
<!--              <a href="https://www.linkedin.com/in/daeil/">Daeil Kim</a>-->
<!--              <br>-->
<!--              <em>WACV</em>, 2023-->
<!--              <br>-->
<!--              <a href="https://aashishrai3799.github.io/3DFaceCAM/">project page</a>-->
<!--              /-->
<!--              <a href="https://arxiv.org/abs/2208.14263">arXiv</a>-->
<!--              /-->
<!--              <a href="https://github.com/aashishrai3799/3DFaceCAM/">code</a>-->
<!--              /-->
<!--              <a href="https://drive.google.com/file/d/1PqIN4Rzp4vapWs2pUegUEoMhg4lM2Smy/view">video</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              A new 3D face generative model that can decouple identity and expression and provides granular control over expressions.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



<!--        &lt;!&ndash;   KAMV    &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/kamv/figure.png", width="165">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548089">-->
<!--                <span class="papertitle">A Knowledge Augmented and Multimodal-Based Framework for Video Summarization</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.semanticscholar.org/author/Jiehang-Xie/147178951">Jiehang Xie*</a>,-->
<!--              <strong>Xuanbai Chen*</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=ObO3hW8AAAAJ">Shao-Ping Lu</a>,-->
<!--              <a href="https://dblp.org/pid/14/1508.html">Yulu Yang</a>-->
<!--              <br>-->
<!--              <em>ACMMM</em>, 2022-->
<!--              <br>-->
<!--              <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548089">project page</a>-->
<!--              /-->
<!--              <a href="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3503161.3548089&file=MM22-1409.mp4">video</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Propose a knowledge augmented and multimodal-based video summarization method, by considering multichannel information and using the impact of external knowledge.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->




<!--        &lt;!&ndash; MANVS  &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/manvs/figure.png", width="180">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/9797228">-->
<!--                <span class="papertitle">Multimodal-based and Aesthetic-guided Narrative Video Summarization</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.semanticscholar.org/author/Jiehang-Xie/147178951">Jiehang Xie</a>,-->
<!--              <strong>Xuanbai Chen</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=k-ogUq0AAAAJ">Tianyi Zhang</a>,-->
<!--              Yixuan Zhang,-->
<!--              <a href="https://scholar.google.com/citations?user=ObO3hW8AAAAJ">Shao-Ping Lu</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=guRMl5IAAAAJ">Pablo Cesar</a>,-->
<!--              <a href="https://dblp.org/pid/14/1508.html">Yulu Yang</a>-->
<!--              <br>-->
<!--              <em>IEEE TMM</em>, 2022-->
<!--              <br>-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/9797228">project page</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--              Introduce a multimodal-based and aesthetic-guided narrative video summarization method, by leveraging multimodal information through specified key shots selection, subtitle summarization, and highlight extraction components.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->




<!--        &lt;!&ndash; CycleEmotionGAN++  &ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td align="center" valign="middle">&nbsp;&nbsp;&nbsp;<img src="images/cycleemotiongan/figure.png", width="200">&nbsp;&nbsp;&nbsp;&nbsp;-->
<!--			</td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/9385998">-->
<!--                <span class="papertitle">Emotional Semantics-Preserved and Feature-Aligned CycleGAN for Visual Emotion Adaptation</span>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://scholar.google.com/citations?user=LJiQRJIAAAAJ">Sicheng Zhao*</a>,-->
<!--              <strong>Xuanbai Chen*</strong>,-->
<!--              <a href="https://scholar.google.com/citations?user=-xQ-C1sAAAAJ">Xiangyu Yue</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=resm5NkAAAAJ">Chuang Lin</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=ck6i0ucAAAAJ&hl=en">Pengfei Xu</a>,-->
<!--              Ravi Krishna,-->
<!--              <a href="https://scholar.google.com/citations?user=c5vDJv0AAAAJ">Jufeng Yang</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=B7F3yt4AAAAJ">Ding Guiguang</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=AhgjQ2QAAAAJ">Alberto Sangiovanni Vincentelli</a>,-->
<!--              <a href="https://scholar.google.com/citations?user=ID9QePIAAAAJ">Kurt Keutzer</a>-->
<!--              <br>-->
<!--              <em>IEEE TCYB</em>, 2021-->
<!--              <br>-->
<!--              <a href="https://ieeexplore.ieee.org/abstract/document/9385998">project page</a>-->
<!--              /-->
<!--              <a href="https://arxiv.org/abs/2011.12470">arxiv</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--                Design a novel end-to-end cycle-consistent adversarial model, to focus on UDA in visual emotion analysis for both emotion distribution learning and dominant emotion classification.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Education and Experiences</heading>
                </td>
            </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <!-- AWS -->
            <tr>
                <td style="padding:20px;width:25%;vertical-align:top" align="center">
                    <img src="images/experiences/aws.jpeg" style="horiz-align: right" width="150px">
                </td>
                <td width="75%" valign="top">
                    <p><a href="https://aws.amazon.com/rekognition/"><strong style="font-size: 18px">AWS AI Rekognition</strong></a>, <strong style="font-size: 18px">Jul. 2023 - Present</strong></p>
                    <p>Applied Scientist</p>
                </td>
            </tr>
            <!-- CMU -->
            <tr>
                <td style="padding:20px;width:25%;vertical-align:top" align="center">
                    <img src="images/experiences/cmu.png" style="horiz-align: right" width="150px">
                </td>
                <td width="75%" valign="top">
                    <p><a href="https://www.cmu.edu/"><strong style="font-size: 18px">Carnegie Mellon University</strong></a>, <strong style="font-size: 18px">Jan. 2022 - May. 2023</strong></p>
                    <p>Master of Science in Computer Vision</p>
                </td>
            </tr>
            <!-- AWS -->
            <tr>
                <td style="padding:20px;width:25%;vertical-align:top" align="center">
                    <img src="images/experiences/aws.jpeg" style="horiz-align: right" width="150px">
                </td>
                <td width="75%" valign="top">
                    <p><a href="https://aws.amazon.com/rekognition/"><strong style="font-size: 18px">AWS AI Rekognition</strong></a>, <strong style="font-size: 18px">May. 2022 - Aug. 2022</strong></p>
                    <p>Applied Scientist Intern</p>
                </td>
            </tr>
<!--            &lt;!&ndash; Sensetime &ndash;&gt;-->
<!--            <tr>-->
<!--                <td style="padding:20px;width:25%;vertical-align:top" align="center">-->
<!--                    <img src="images/experiences/sensetime.jpeg" style="horiz-align: right" width="150px">-->
<!--                </td>-->
<!--                <td width="75%" valign="top">-->
<!--                    <p><a href="https://www.sensetime.com/en"><strong style="font-size: 18px">Sensetime</strong></a>, <strong style="font-size: 18px">Aug. 2021 - Dec. 2021</strong></p>-->
<!--                    <p>Research Engineer Intern</p>-->
<!--                </td>-->
<!--            </tr>-->
            <!-- UCB -->
            <tr>
                <td style="padding:20px;width:25%;vertical-align:top" align="center">
                    <img src="images/experiences/berkeley.png" style="horiz-align: right" width="150px">
                </td>
                <td width="75%" valign="top">
                    <p><a href="https://www.berkeley.edu/"><strong style="font-size: 18px">University of California, Berkeley</strong></a>, <strong style="font-size: 18px">Jan. 2020 - May. 2020</strong></p>
                    <p>Exchange Program</p>
                </td>
            </tr>
            <!-- Nankai -->
            <tr>
                <td style="width:25%;vertical-align:top;horiz-align: right" align="center">
                    <img src="images/experiences/nankai.png" style="horiz-align: right" width="150px">
                </td>
                <td width="75%" style="vertical-align:top">
                    <p><a href="https://en.nankai.edu.cn/"><strong style="font-size: 18px">Nankai University</strong></a>, <strong style="font-size: 18px">Sep. 2017 - Jun. 2021</strong></p>
                    <p>Bachelor of Engineering in Computer Science</p>
                </td>
            </tr>
            </tbody>
        </table>



<!--        &lt;!&ndash;academic service&ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Academic Services</heading>-->
<!--                <p>-->
<!--                  <li style="margin: 5px;">-->
<!--                    <b>Journal Reviewer:</b>  IEEE TCYB, TMM, TNSRE, KBS, TMLR-->
<!--                  </li>-->
<!--                  <li style="margin: 5px;">-->
<!--                    <b>Conference Reviewer:</b>  ACMMM 2023 2024 2025, CVPR 2024, ECCV 2024, ICCV 2025, AAAI 2025-->
<!--                  </li>-->
<!--                  <li style="margin: 5px;">-->
<!--                    <b>Student Volunteer:</b>  ICCV 2023-->
<!--                  </li>-->
<!--                </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Miscellaneous</heading>
                <p align="justify">
                    The initial label for me is a sports fan and I love watching various sports games in my leisure time. My favorite players are LeBron James, Lionel Messi, Roger Federer for basketball, football, and tennis respectively.
                </p>
                <p align="justify">
                    Furthermore, I will also spend some time in watching popularization videos in different subjects to prevent me from being 'ignorant' in other fields.
                    <!-- Check my <a href="https://xuanbai-chen.github.io/">blogs</a> which show my perspective for different things.-->
                </p>
            </td>
          </tr>
        </tbody></table>

    </td></tr>
  </tbody></table>

    <p>
      <center>
          <div id="clustrmaps-widget" style="width:20%">
            <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=_4wnmcZthdDL7Y20ecaEP9y8aJ2WGqD1dpAjBwIP6uU&cl=ffffff&w=a"></script>
          </div>
          <br>&copy; Xuanbai Chen | Last updated: Mar 21, 2025 | Jon Barron's webpage template
      </center>
    </p>

  </body>
</html>